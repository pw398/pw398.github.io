---
layout: post
title:  A Visual Understanding of Latent Dirichlet Allocation (LDA)
date:   2025-07-13 00:00:00 +0000
categories: TopicModeling LDA
---


Latent Dirichlet Allocation (LDA) is an unsupervised clustering method, largely used for the topic-modeling of text documents. While it produces a very rich interpretation, this comes with some complexity. The aim of this article is to provide visual and intuitive understanding, setting the stage for some real-word application in the next article.

<img src="https://raw.githubusercontent.com/pw398/pw398.github.io/main/_posts/images/library.jpg" style="height: 600px; width:auto;">



# Outline 

- Introduction
- Intuition
- The Dirichlet Distribution
  - The Stick-Breaking Analogy
- Plate Notation
  - The Unigram Model
  - Mixture of Unigrams
  - LDA
- Further Python Visualization
- Solving LDA
- What's Next?



# Introduction

Introduced in <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">this paper</a> by David Blei et al., Latent Dirichlet Allocation (LDA) is an unsupervised clustering method commonly used for topic-modeling of documents in a corpus (collection of documents). Because it is unsupervised, labels are neither required as input, nor produced explicitly as output. We do provide the number of topics $K$, expecting to find meaningful distinctions and associations (and adjusting our approach if not). LDA is a soft-clustering method, so unlike the crisp topic-predictions we obtain from a method like K-Means, it assigns a probability distribution over topics to each document.

As a quick sidenote, the 'LDA' we're referring to is not related to the dimension-reduction technique Linear Discriminant Analysis, which shares the same acronym.

<p>Latent Dirichlet Allocation is a generative model. Rather than a discriminative model, which considers conditional probability $P(Y|X)$ given observed data $X$, we model the joint probability $P(X,Y)$, with $X$ representing the observed data, and $Y$ representing latent (hidden) variables. In solving, we actually generate sample documents, mixtures of topics and words drawn from topic-specific distributions. The generated documents are nonsensical in terms of our plain-language method of interpretation, as we use the bag-of-words (BoW) approach to vectorization, counting word frequencies, but not considering word sequences. Nevertheless, the model tends to perform quite well, and delivers a very rich interpretation of results, compared to other common methods of topic-modeling. We can (and next article, will) collapse the distributions into class predictions, like a supervised method (referred to as sLDA).</p>



# Intuition

As a soft-clustering method, LDA facilitates overlapping topic assignments. Rather than each document belonging to exactly one cluster, LDA assigns:

<ol>
  <li>A probability distribution over topics to each document.</li>
  <ul>
    <li>e.g., a document is 40% represented by fiction, 20% by science, and so on with regard to other topics.</li>
  </ul>
  <li>A probability distribution over words to each topic.</li>
  <ul>
    <li>e.g., a topic is represented 5% by "quantum", 2% by "teleport", and so on with regard to other words (perhaps thousands of them).</li>
  </ul>
</ol>

Imagine a library where many books are strewn across the tables and floor, with no labels or organization. This collection of books represents our corpus. You need to create an algorithm to sort them into sections, but you do not know which categorizations exist beforehand. The books are our documents, the unknown categorizations are our topics, and the words are clues as to which topic a book belongs to. If a book has words like "experiment" and "hypothesis", the latent category corresponding to 'science' may be the one it predominantly belongs to. It may also have a strong probability of belonging to 'biology' or 'physics', and have small (often very small) probabilities of belonging to many other categories.

<p>Most versions of LDA are generative models. Rather than consider the conditional probability $P(Y|X)$ given observed data $X$, as with a discriminative model, we model the joint probability $P(X,Y)$, with $Y$ representing latent (hidden) variables. In solving, we generate sample documents, which are nonsensical in plain language, because we utilize the bag-of-words (BoW) approach to text vectorization, paying attention to word frequencies, but not sequences. Nevertheless, we tend to come up with an effective predictor, which provides greater context than other common methods. We can (and next article, will) collapse the distributions into class predictions like a supervised method (called sLDA), and then compare to other models on a test set of data.</p>

I promise I won't subject you (or I) to too much math, but some is necessary to describe the Dirichlet and related distributions. After all, it's in the name. Feel free to skim over it and focus more on the visuals further below.

